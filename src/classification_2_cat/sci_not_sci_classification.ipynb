{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e4d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/steve/Desktop/ML/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold#, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79231929",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df_path= \"../../scitweets.tsv\"\n",
    "df = pd.read_csv(df_path, sep='\\t', header=0)\n",
    "\n",
    "df = df.drop(columns=[col for col in df.columns if \"Unnamed\" in col] + [\"tweet_id\"])\n",
    "df[\"text\"] = df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49272cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiExtractor(BaseEstimator, TransformerMixin):\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\n",
    "\t\tdef count_emojis(text):\n",
    "\t\t\treturn sum(1 for char in text if char in emoji.EMOJI_DATA)\n",
    "\n",
    "\t\tdef replace_emojis(text):\n",
    "\t\t\treturn emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "\t\tX[\"emoji_count\"] = X[\"text\"].apply(count_emojis)\n",
    "\t\tX[\"text\"] = X[\"text\"].apply(replace_emojis)\n",
    "\n",
    "\t\treturn X\n",
    "\n",
    "class LinkExtractor(BaseEstimator, TransformerMixin):\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\n",
    "\t\tdef count_links(text):\n",
    "\t\t\treturn len(re.findall(r'https?:\\/\\/.*[\\r\\n]*', text))\n",
    "\n",
    "\t\tdef remove_links(text):\n",
    "\t\t\treturn re.sub(r'https?:\\/\\/\\S+', '[url]', text)\n",
    "\n",
    "\t\tX[\"link_count\"] = X[\"text\"].apply(count_links)\n",
    "\t\tX[\"text\"] = X[\"text\"].apply(remove_links)\n",
    "\n",
    "\t\treturn X\n",
    "\t\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\t\tdef preprocess(text):\n",
    "\t\t\tdoc = nlp(text.lower())\n",
    "\t\t\treturn \" \".join(token.lemma_ for token in doc if not token.is_stop and not token.is_punct)\n",
    "\n",
    "\t\tX[\"text\"] = X[\"text\"].apply(preprocess)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581dba6",
   "metadata": {},
   "source": [
    "## Define the pipeline for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f099e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>.@Frontier_Usyd is hosting a Frontotemporal Dementia (FTD) Information &amp; Support Day for Families and Care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>If only people would stop interfering in other people's business ðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>when will the underwear stop flying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>@martinhanratty @AtyHans @thenotimer I listened to a gospel radio station for a couple of hours--the confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>when truth untold stops playing and the lights turn green.... https://t.co/lBFhLHyUbn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              text\n",
       "938  .@Frontier_Usyd is hosting a Frontotemporal Dementia (FTD) Information & Support Day for Families and Care...\n",
       "470                                      If only people would stop interfering in other people's business ðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œðŸ˜œ\n",
       "326                                                                            when will the underwear stop flying\n",
       "406  @martinhanratty @AtyHans @thenotimer I listened to a gospel radio station for a couple of hours--the confi...\n",
       "953                          when truth untold stops playing and the lights turn green.... https://t.co/lBFhLHyUbn"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.copy().drop(columns=[\"scientific_claim\", \"scientific_reference\", \"scientific_context\"])\n",
    "y = X.pop(\"science_related\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, test_size=0.2)\n",
    "pd.set_option('display.max_colwidth', 110)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96811519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(predicted_y, true_y ):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    cm = confusion_matrix(true_y, predicted_y)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non science related\", \"Science related\"],\n",
    "            yticklabels=[\"Non science related\", \"Science related\"], ax=axes[0])\n",
    "\n",
    "    axes[0].set_xlabel(\"Predicted Label\")\n",
    "    axes[0].set_ylabel(\"True Label\")\n",
    "    axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "    target_names = ['Non science related', 'Science related']\n",
    "    class_report = classification_report(true_y, predicted_y, labels=[0, 1], target_names=target_names)\n",
    "\n",
    "    axes[1].text(0, 0.5, class_report, fontsize=12, family='monospace')\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Classification Report\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67af0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words='english', ngram_range=(1,4)), \"text\"),\n",
    "    (\"pass_features\", \"passthrough\", [\"emoji_count\", \"link_count\"])\n",
    "])\n",
    "\n",
    "\n",
    "def getPipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"emoji_processing\", EmojiExtractor()),\n",
    "        (\"link_processing\", LinkExtractor()),\n",
    "        (\"text_processing\", TextPreprocessor()),\n",
    "        (\"feature_vectorizer\", column_transformer),\n",
    "        (\"classifier\", model)\n",
    "        #(\"classifier\", svm.SVC(kernel='linear', class_weight='balanced'))\n",
    "        #(\"classifier\", RandomForestClassifier(random_state = 1))\n",
    "        #(\"classifier\", XGBRegressor( random_state=1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:25:52,972] A new study created in memory with name: no-name-937213c1-b13b-47e5-808a-31507f5355a6\n",
      "/tmp/ipykernel_38142/2088243869.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1, 10.0)\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "  C = trial.suggest_loguniform('C', 1, 10.0)\n",
    "\n",
    "  kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
    "  gamma = trial.suggest_loguniform('gamma', 7, 10.0) if kernel == 'rbf' else 'scale'\n",
    "  model = svm.SVC(C=C, kernel=kernel, gamma=gamma, random_state=1, class_weight='balanced')\n",
    "\n",
    "  pipeline = getPipeline(model)\n",
    "\n",
    "  kfold = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "  score = cross_val_score(pipeline, X=X_train, y=y_train,  cv=kfold, scoring='f1_macro').mean()\n",
    "\n",
    "  return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleurs hyperparamÃ¨tres :\")\n",
    "print(study.best_params)\n",
    "print(f\"Meilleur score : {study.best_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm. SVC(\n",
    "  C=study.best_params['C'],\n",
    "  kernel=study.best_params['kernel'],\n",
    "  random_state=1,\n",
    "  class_weight='balanced'\n",
    "  )\n",
    "pipeline = getPipeline(model)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0978e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "display_result(predictions, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac090d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    pipeline = getPipeline(model)\n",
    "\n",
    "    kfold = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "    score = cross_val_score(pipeline, X=X_train, y=y_train, cv=kfold, scoring='f1_macro').mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "        n_estimators=study.best_params['n_estimators'],\n",
    "        max_depth=study.best_params['max_depth'],\n",
    "        min_samples_split=study.best_params['min_samples_split'],\n",
    "        min_samples_leaf=study.best_params['min_samples_leaf'],\n",
    "        random_state=1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "pipeline = getPipeline(model)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70680240",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "display_result(predictions, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(predictions != y_test)[0]\n",
    "\n",
    "# RÃ©cupÃ©rer les tweets mal classifiÃ©s\n",
    "misclassified_tweets = X_test.iloc[misclassified_indices]['text'].tolist()\n",
    "misclassified_true = y_test.iloc[misclassified_indices].tolist()\n",
    "misclassified_pred = predictions[misclassified_indices].tolist()\n",
    "\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'tweet': misclassified_tweets,\n",
    "    'true_label': misclassified_true,\n",
    "    'predicted_label': misclassified_pred\n",
    "})\n",
    "misclassified_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = pipeline.named_steps['feature_vectorizer'].named_transformers_['tfidf']\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = tfidf_vectorizer.transform(X['text'])\n",
    "feature_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
    "\n",
    "feature_df = pd.DataFrame({'word': feature_names, 'tfidf_score': feature_scores})\n",
    "\n",
    "feature_df.sort_values(by='tfidf_score', ascending=False).head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
