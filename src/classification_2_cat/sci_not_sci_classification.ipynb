{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "%pip install emoji\n",
    "import emoji\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "%pip install optuna\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold#, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79231929",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv(df_url, sep='\\t', header=0)\n",
    "df = df.drop(columns=[col for col in df.columns if \"Unnamed\" in col] + [\"tweet_id\"])\n",
    "df[\"text\"] = df[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49272cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiExtractor(BaseEstimator, TransformerMixin):\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\n",
    "\t\tdef count_emojis(text):\n",
    "\t\t\treturn sum(1 for char in text if char in emoji.EMOJI_DATA)\n",
    "\n",
    "\t\tdef replace_emojis(text):\n",
    "\t\t\treturn emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "\t\tX[\"emoji_count\"] = X[\"text\"].apply(count_emojis)\n",
    "\t\tX[\"text\"] = X[\"text\"].apply(replace_emojis)\n",
    "\n",
    "\t\treturn X\n",
    "\n",
    "class LinkExtractor(BaseEstimator, TransformerMixin):\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\n",
    "\t\tdef count_links(text):\n",
    "\t\t\treturn len(re.findall(r'https?:\\/\\/.*[\\r\\n]*', text))\n",
    "\n",
    "\t\tdef remove_links(text):\n",
    "\t\t\treturn re.sub(r'https?:\\/\\/\\S+', '[url]', text)\n",
    "\n",
    "\t\tX[\"link_count\"] = X[\"text\"].apply(count_links)\n",
    "\t\tX[\"text\"] = X[\"text\"].apply(remove_links)\n",
    "\n",
    "\t\treturn X\n",
    "\t\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\t\tdef preprocess(text):\n",
    "\t\t\tdoc = nlp(text.lower())\n",
    "\t\t\treturn \" \".join(token.lemma_ for token in doc if not token.is_stop and not token.is_punct)\n",
    "\n",
    "\t\tX[\"text\"] = X[\"text\"].apply(preprocess)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581dba6",
   "metadata": {},
   "source": [
    "## Define the pipeline for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f099e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy().drop(columns=[\"scientific_claim\", \"scientific_reference\", \"scientific_context\"])\n",
    "y = X.pop(\"science_related\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, test_size=0.2)\n",
    "pd.set_option('display.max_colwidth', 110)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96811519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(predicted_y, true_y ):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    cm = confusion_matrix(true_y, predicted_y)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non science related\", \"Science related\"],\n",
    "            yticklabels=[\"Non science related\", \"Science related\"], ax=axes[0])\n",
    "\n",
    "    axes[0].set_xlabel(\"Predicted Label\")\n",
    "    axes[0].set_ylabel(\"True Label\")\n",
    "    axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "    target_names = ['Non science related', 'Science related']\n",
    "    class_report = classification_report(true_y, predicted_y, labels=[0, 1], target_names=target_names)\n",
    "\n",
    "    axes[1].text(0, 0.5, class_report, fontsize=12, family='monospace')\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Classification Report\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words='english', ngram_range=(1,4)), \"text\"),\n",
    "    (\"pass_features\", \"passthrough\", [\"emoji_count\", \"link_count\"])\n",
    "])\n",
    "\n",
    "\n",
    "def getPipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"emoji_processing\", EmojiExtractor()),\n",
    "        (\"link_processing\", LinkExtractor()),\n",
    "        (\"text_processing\", TextPreprocessor()),\n",
    "        (\"feature_vectorizer\", column_transformer),\n",
    "        (\"classifier\", model)\n",
    "        #(\"classifier\", svm.SVC(kernel='linear', class_weight='balanced'))\n",
    "        #(\"classifier\", RandomForestClassifier(random_state = 1))\n",
    "        #(\"classifier\", XGBRegressor( random_state=1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  C = trial.suggest_loguniform('C', 1, 10.0)\n",
    "\n",
    "  kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
    "  gamma = trial.suggest_loguniform('gamma', 7, 10.0) if kernel == 'rbf' else 'scale'\n",
    "  model = svm.SVC(C=C, kernel=kernel, gamma=gamma, random_state=1, class_weight='balanced')\n",
    "\n",
    "  pipeline = getPipeline(model)\n",
    "\n",
    "  kfold = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "  score = cross_val_score(pipeline, X=X_train, y=y_train,  cv=kfold, scoring='f1_macro').mean()\n",
    "\n",
    "  return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(study.best_params)\n",
    "print(f\"Meilleur score : {study.best_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm. SVC(\n",
    "  C=study.best_params['C'],\n",
    "  kernel=study.best_params['kernel'],\n",
    "  random_state=1,\n",
    "  class_weight='balanced'\n",
    "  )\n",
    "pipeline = getPipeline(model)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0978e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "display_result(predictions, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac090d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    pipeline = getPipeline(model)\n",
    "\n",
    "    kfold = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "    score = cross_val_score(pipeline, X=X_train, y=y_train, cv=kfold, scoring='f1_macro').mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "        n_estimators=study.best_params['n_estimators'],\n",
    "        max_depth=study.best_params['max_depth'],\n",
    "        min_samples_split=study.best_params['min_samples_split'],\n",
    "        min_samples_leaf=study.best_params['min_samples_leaf'],\n",
    "        random_state=1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "pipeline = getPipeline(model)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70680240",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "display_result(predictions, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(predictions != y_test)[0]\n",
    "\n",
    "# Récupérer les tweets mal classifiés\n",
    "misclassified_tweets = X_test.iloc[misclassified_indices]['text'].tolist()\n",
    "misclassified_true = y_test.iloc[misclassified_indices].tolist()\n",
    "misclassified_pred = predictions[misclassified_indices].tolist()\n",
    "\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'tweet': misclassified_tweets,\n",
    "    'true_label': misclassified_true,\n",
    "    'predicted_label': misclassified_pred\n",
    "})\n",
    "misclassified_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = pipeline.named_steps['feature_vectorizer'].named_transformers_['tfidf']\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = tfidf_vectorizer.transform(X['text'])\n",
    "feature_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n",
    "\n",
    "feature_df = pd.DataFrame({'word': feature_names, 'tfidf_score': feature_scores})\n",
    "\n",
    "feature_df.sort_values(by='tfidf_score', ascending=False).head(20)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
